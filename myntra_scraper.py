# üì¶ Standard libraries
import asyncio
import re, os, random, json

# üì¶ Third-party libraries
from playwright.async_api import async_playwright  # Async Playwright for browser automation
from playwright_stealth import stealth_async       # Helps bypass bot detection
import pandas as pd     

from asyncio import Semaphore                           # For saving scraped data to Excel
from more_itertools import chunked  # pip install more-itertools

# üåê Proxy list - rotating proxies to reduce chance of IP bans
proxies = [
    {"server": "isp.decodo.com:10001", "username": "spavxwrl7o", "password": "d5_aqvaCG3eacVi1P8"},
    {"server": "isp.decodo.com:10002", "username": "spavxwrl7o", "password": "d5_aqvaCG3eacVi1P8"},
    {"server": "isp.decodo.com:10003", "username": "spavxwrl7o", "password": "d5_aqvaCG3eacVi1P8"}
]

# üß† Create a browser context with randomized settings to mimic real users
async def create_dynamic_context(browser):
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36",
        "Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15"
    ]
    locales = ["en-US", "en-GB", "hi-IN", "fr-FR"]
    timezones = ["Asia/Kolkata", "America/New_York", "Europe/London", "Asia/Tokyo"]
    viewports = [{"width": 1280, "height": 800}]

    # üé≠ Create a new browser context with randomized settings
    context = await browser.new_context(
        user_agent=random.choice(user_agents),
        locale=random.choice(locales),
        timezone_id=random.choice(timezones),
        viewport=random.choice(viewports)
    )
    return context

# ‚úÖ A safe wrapper around page.goto with retries and backoff
async def safe_goto(page, url, retries=3):
    for i in range(retries):
        try:
            await page.goto(url, timeout=60000, wait_until="domcontentloaded")
            return True
        except Exception as e:
            print(f"‚ö†Ô∏è Retry {i+1}/{retries} for {url} due to error: {e}")
            await asyncio.sleep(2 ** i)  # Wait 1, 2, 4... seconds
    return False

# üß≤ Core scraping function to collect product links from a category page
async def scrape_category(page, category_url):
    success = await safe_goto(page, category_url)
    if not success:
        print(f"‚ùå Failed to load {category_url}")
        return []

    # Random wait to mimic human behavior
    await asyncio.sleep(random.uniform(5, 10))

    # Wait for product container to load
    await page.wait_for_selector('ul.results-base > li', timeout=10000)

    # üîÉ Scroll down to trigger lazy loading
    for _ in range(10):
        await page.mouse.wheel(0, random.randint(100, 300))
        await page.wait_for_timeout(random.randint(500, 1000))

    await asyncio.sleep(random.uniform(3, 6))

    # Extract all product links
    li_elements = await page.query_selector_all("ul.results-base > li.product-base")
    product_links = []
    for li in li_elements:
        a_tag = await li.query_selector("a")
        href = await a_tag.get_attribute("href") if a_tag else None
        if href:
            product_links.append({'Product_Link': f"https://www.myntra.com/{href}"})

    print(f"üß≠ Found {len(product_links)} links on this page.")
    return product_links

# üëá Accept playwright, not browser
async def scrape_product_details(context, link, proxy, semaphore):
    async with semaphore:
        await asyncio.sleep(random.uniform(1, 3))

        # browser = await playwright.chromium.launch(headless=False)

        # context = await browser.new_context(
        #     user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        #     locale="en-US",
        #     timezone_id="Asia/Kolkata",
        #     viewport={"width": 1280, "height": 800},
        #     proxy={
        #         "server": f"http://{proxy['server']}",
        #         "username": proxy["username"],
        #         "password": proxy["password"]
        #     }
        # )
        page = await context.new_page()
        try:
            await page.goto(link['Product_Link'])
            await asyncio.sleep(random.uniform(2, 4))
            await page.wait_for_selector("h1.pdp-title", timeout=5000)
            title = await page.text_content("h1.pdp-title")
            description = await page.text_content("h1.pdp-name")

            try:
                rating = await page.text_content("div.index-overallRating > div:nth-child(1)")
            except:
                rating = ""

            try:
                numberOfRatings = await page.text_content("div.index-ratingsCount")
            except:
                numberOfRatings = ""

            link['Title'] = title
            link['Description'] = description
            link['Ratings'] = rating
            link['Number Of Ratings'] = numberOfRatings

        except Exception as e:
            print(f"Error scraping {link['Product_Link']}: {e}")
        # finally:
        #     await context.close()
        #     # await browser.close()


# üöÄ Main function to manage the scraping process
async def main():
    category_urls = [
        # Add more categories here
        "https://www.myntra.com/bra"
    ]
    total_pages_to_scrape = 1
    all_results = []

    # üé¨ Start Playwright session
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)  # Set to True for background scraping

        for category_url in category_urls:
            for page_num in range(1, total_pages_to_scrape + 1):
                paginated_url = f"{category_url}?p={page_num}"
                print(f"üìÑ Scraping: {paginated_url}")

                try:
                    # Rotate proxy for every request
                    proxy = random.choice(proxies)

                    # üß± Create a new context for each page (mimics new user session)
                    context = await browser.new_context(
                        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                        locale="en-US",
                        timezone_id="Asia/Kolkata",
                        viewport={"width": 1280, "height": 800},
                        proxy={
                            "server": f"http://{proxy['server']}",
                            "username": proxy["username"],
                            "password": proxy["password"]
                        }
                    )

                    page = await context.new_page()

                    # Add headers to look less like a bot
                    await page.set_extra_http_headers({
                        "Accept-Language": "en-US,en;q=0.9",
                        "Referer": "https://www.google.com/"
                    })

                    await asyncio.sleep(random.uniform(3, 6))

                    # üîç Scrape this page
                    links = await scrape_category(page, paginated_url)
                    all_results.extend(links)

                    # Clean up
                    await page.close()
                    # await context.close()
                    await asyncio.sleep(random.uniform(1, 2))

                except Exception as e:
                    print(f"‚ùå Error scraping {paginated_url}: {e}")
            # Now get all the required details by visiting each link
            all_results = all_results[:4]
            # Run up to 5 scrapers at a time in parallel
            # Limit parallelism (e.g., max 5 concurrent scrapers)
            semaphore = asyncio.Semaphore(2)
            batch_size = 2
            for batch in chunked(all_results, batch_size):
                context = await browser.new_context(
                        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                        locale="en-US",
                        timezone_id="Asia/Kolkata",
                        viewport={"width": 1280, "height": 800},
                        proxy={
                            "server": f"http://{proxy['server']}",
                            "username": proxy["username"],
                            "password": proxy["password"]
                        }
                    )
                tasks = [
                    scrape_product_details(context, link, random.choice(proxies), semaphore)
                    for link in batch
                ]
                await asyncio.gather(*tasks)
                print(f"‚úÖ Completed one batch of {len(batch)} tasks.")
                await context.close()
                await asyncio.sleep(random.uniform(10, 20))  # delay before next batch
        await context.close()
        await browser.close()

    # üíæ Save the collected product links to Excel
    df = pd.DataFrame(all_results, columns=["Product_Link","Title","Description","Ratings","Number Of Ratings"])
    df.to_excel("myntra_bra_links.xlsx", index=False)
    print("‚úÖ Saved to myntra_bra_links.xlsx")

# üßµ Entry point of the script
if __name__ == "__main__":
    asyncio.run(main())  # üîÅ Run the async main function inside event loop
